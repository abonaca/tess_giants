\documentclass[modern]{aastex63}
\usepackage{amsmath}

% typography
\usepackage[T1]{fontenc}
\setlength{\parindent}{1.\baselineskip}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\package}[1]{\textsl{#1}}
\newcommand{\changes}[1]{{\textbf{#1}}}
\renewcommand{\twocolumngrid}{} % This is evil Hogg stuff right here.

% math
\newcommand{\nupeak}{\nu_\mathrm{peak}}
\newcommand{\numax}{\nu_\mathrm{max}}
\newcommand{\hquad}{~~}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\vy}{\vector{y}}
\newcommand{\veta}{\vector{\eta}}
\newcommand{\vtheta}{\vector{\theta}}
\renewcommand{\tensor}[1]{\mathbf{#1}}
\newcommand{\tB}{\tensor{B}}
\newcommand{\tC}{\tensor{C}}
\DeclareMathOperator{\diag}{diag}
\newcommand{\tLambda}{\tensor{\Lambda}}
\newcommand{\mat}[1]{\mathsf{#1}}
\newcommand{\mM}{\mat{M}}
\newcommand{\normal}{\mathcal{N}\!\,}

% aastex parameters
% \received{not yet; THIS IS A DRAFT}
%\revised{not yet}
%\accepted{not yet}
% % Adds "Submitted to " the argument.
% \submitjournal{ApJ}
\shorttitle{}
\shortauthors{bonaca \& hogg}

%@arxiver{}
\usepackage{amsmath}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust me

\title{Likelihood for the forest of asteroseismic modes}

\correspondingauthor{Ana~Bonaca}
\email{ana.bonaca@cfa.harvard.edu}

\author[0000-0002-7846-9787]{Ana~Bonaca}
\affil{Center for Astrophysics | Harvard \& Smithsonian, 60 Garden Street, Cambridge, MA 02138, USA}

\author[0000-0003-2866-9403]{David~W.~Hogg}
\affiliation{Center for Cosmology and Particle Physics, Department of Physics, New York University}
\affiliation{Center for Data Science, New York University}
\affiliation{Max-Planck-Institut f\"ur Astronomie, Heidelberg}
\affiliation{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Avenue, NY 10010, USA}

\begin{abstract}\noindent % trust me
Foo and bar.
\end{abstract}

\section{Introduction}
\label{sec:intro}

- successes of asteroseismology

- possible to do bc kepler and tess come close to ideal dataset for asteroseismology: sampling good, timebaseline sufficient to resolve modes
%- no data exactly that, but kepler and tess come close, therefore resolving modes
- typical workflow: fft + peak bagging

- dream: do large-scale asteroseismic missions
- while asteroseismic missions getting larger themselves, e.g., tess, plato, a wealth of time domain data at different cadence
- some work done (cite atlas/asassn paper), but much more possible if able to deal w gaia / lsst cadence
-> need generative model

- sth about why generative model hard / not done so far in asteroseismology (physical model hard / don't have a good theory of amplitudes? so doing fft bc can get frequencies alone easily)
- examples of contexts where generative model worked, why it might be a good idea to try here
- benefits of being able to do inference:
-- can use even suboptimally sampled data, just will get noisier constraints
-- test properties of stars

- luminous red giants valuable for mapping the galaxy
- high amplitude, coherent models -- good asteroseismic targets too, especially for less precise photometry
- model treating modes as coherent in data stream, but they are not in the tess window for sun-like stars, geared towards giants where coherence times are long (would need to use a proper gp or cut data into pieces for sunlike stars)
- not solving all problems for all people, but can do this specific regime
- can take more data than before, maybe map the whole galaxy
- be modest on what we solve
- part of a big project that astronomy should solve

- paper plan:
2) develop likelihood
3) test on aguirre sample: identify where works well and where not
4) blind search: here they are + hopefully make sense
5) discussion


figures:
sec 2: pedagogical -- labeled powerspectrum w nupeak, dnu, bell params 
sec 2: row 1: schematic of different models (just the comb, comb + nupeak marginalization, comb + bell); row 2: likelihood surfaces of the same lightcurve under these models
sec 3: compare likelihood + powerspectra on the bottom for different regimes
sec 4: HRD color-coded by inferre dnu, numax, etc


- beginning of the methods section: two likelihoods we could use
1) comprehensive to include all things we know about stars
2) quick, can search computationally, but doesn't have all the inputs (e.g., no odd/even l, or taking care of them in a weird way to avoid having a parameter, filtering, hard to apply to unevenly sampled streams, like gaia -- new kind of data might require new kinds of hacks if we want something computationally fast)


discussion:
-- talk about all kinds of data regimes: tess (very good data, short timebaseline, slightly gappy), ground-based (very good sampling, rv spec | sampling horrible, like lsst), space-based (precision great, sampling bad, gaia) -- regimes where fft troublesome (nobody has a plan for gaia, even thinks its possible)
-- regimes of timescales (min dt between observations, max T, baseline)
-- in asteroseismic spectrum relevant timescales: 1/numax, 1/dnu, coherence time (1/true width of the modes) -- very separated, so a lot of datasets are in between (giants probably not even resolved in kepler, coherence time longer than 4 yrs -- spinoff undergrad paper for kepler: can we measure coherence time for giant modes)


\section{Data}\label{sec:data}

... Words here ...

\section{Likelihood and marginalized likelihood}\label{sec:lhood}

There are many ways to construct a likelihood function for an
asteroseismic-like frequency comb that generates a light curve.
We will build up in stages of increasing sophistication.
The simplest idea is that the light curve is generated by a set (``comb'') of
$K$ equally spaced frequencies.

Presume that there are $N$ data points (normalized flux measurements)
$y_n$, normalized and then unit-subtracted (or high-pass filtered??)
to have a mean very near zero, and a quantitative interpretation in terms
of fractional variation of the star.
The data points are taken at (precisely known) times $t_n$.
Each of these flux measurements has some kind of uncertainty estimate
$\sigma_n$ associated with it and inverse variance $\sigma_n^{-2}$.
The simplest $K$-frequency model is that these flux measurements are
generated by a set of $K$ frequencies $\nu_k$ (where $K$ is odd) as follows:
\begin{align}
  y_n &= f(t_n) + \mbox{noise}
  \\
  f(t_n) &= a_0 + \sum_{k=1}^K \left[a_k\,\cos(2\pi\,\nu_k\,t_n) + b_k\,\sin(2\pi\,\nu_k\,t_n)\right]
  \\ \label{eq:nuk}
  \nu_k &= \nupeak + \left(k - \frac{K + 1}{2}\right)\,\Delta\nu
  \quad \mbox{for $1\leq k\leq K$, $K$ odd,}
\end{align}
where $\nupeak$ is the position of one line (preferably a central line)
and $\Delta\nu$ is the asteroseismic ``large'' frequency difference.
In this formulation, $a_0$ and the $a_k,b_k$ comprise a vector of
$2\,K+1$ linear parameters and $\nupeak, \Delta\nu$ comprise a
blob of 2 nonlinear parameters.
We assemble the linear and nonlinear parameters into column vectors $\vtheta,\veta$
\begin{align}
  \vtheta &\equiv \begin{bmatrix} a_0 & a_1 & b_1 & a_2 & b_2 & \hdots & a_K & b_K \end{bmatrix}\T
  \\
  \veta &\equiv \begin{bmatrix} \Delta\nu & \nupeak & \hdots & K \end{bmatrix}\T
  \quad ,
\end{align}
where the ellipsis indicates that we will be augmenting the
nonlinear-parameter vector $\veta$ in a moment.

Under the assumption of Gaussian noise, the basic log likelihood $\ln L_0$ looks like a
chi-squared:
\begin{align}\label{eq:like}
  \ln L_0(\vtheta,\veta)
  &= -\frac{1}{2}\,\sum_{n=1}^N \frac{[y_n - f(t_n)]^2}{\sigma_n^2}
     -\frac{1}{2}\,\sum_{n=1}^N \ln(2\pi\,\sigma_n^2)
  \quad,
\end{align}
where we have included the $\ln\sigma^2$ terms because they will reappear later.
The basic likelihood is a function of both the linear parameters $\vtheta$ and
the nonlinear parameters $\veta$ because the $f(t)$ functions depend on these
parameters, and nothing has been marginalized out (yet).
The optimization of the basic log likelihood $\ln L_0$
over the linear parameters $a_0$ and the $a_k, b_k$, and the nonlinear
parameters $K, \nupeak, \Delta\nu$, would by itself provide a
classical-statistics estimate of the large frequency difference
$\Delta\nu$.

Acting on real data, this likelihood function is a mind-bogglingly complex
function of the nonlinear parameters.
For this reason, the only safe optimization method in $\Delta\nu, \nupeak$ is
brute-force search!
We perform brute-force search for example light curves  and show the results in XXX
and YYY.
For signal-processing reasons, the brute-force grid in grid search must be
substantially finer than a frequency resolution of $\delta\nu = 1/(K\,T)$ where
$K$ is the number of modes in the comb, and $T$ is the full time span of the
light curve data ($T = t_N - t_1$).
That hurts.

If we define a few linear-algebra operators, we can write the likelihood in a more
generalizable form.
We construct a N-vector (column vector) $\vy$, a diagonal square noise
variance tensor $\tC$, and a rectangular design matrix $\mM$:
\begin{align}
  \vy &\equiv \begin{bmatrix} y_1 & y_2 & y_3 & \hdots & y_N \end{bmatrix}\T
  \\
  \tC &\equiv \diag(\begin{bmatrix} \sigma_1^2 & \sigma_2^2 & \sigma_3^2 & \hdots & \sigma_N^2 \end{bmatrix})
  \\
  \mM &\equiv \begin{bmatrix}
    1 & \cos(2\pi\nu_1 t_1) & \sin(2\pi\nu_1 t_1) & \cos(2\pi\nu_2 t_1) & \hdots & \sin(2\pi\nu_K t_1) \\
    1 & \cos(2\pi\nu_1 t_2) & \sin(2\pi\nu_1 t_2) & \cos(2\pi\nu_2 t_2) & \hdots & \sin(2\pi\nu_K t_2) \\
    1 & \cos(2\pi\nu_1 t_3) & \sin(2\pi\nu_1 t_3) & \cos(2\pi\nu_2 t_3) & \hdots & \sin(2\pi\nu_K t_3) \\
    \vdots & \vdots         & \vdots              & \vdots              &        & \vdots \\
    1 & \cos(2\pi\nu_1 t_N) & \sin(2\pi\nu_1 t_N) & \cos(2\pi\nu_2 t_N) & \hdots & \sin(2\pi\nu_K t_N) \\
  \end{bmatrix}
  \quad ,
\end{align}
where the $\diag()$ operator makes a diagonal matrix with the inputs down
the diagonal.
With these definitions, the basic log likelihood becomes
\begin{align}
  \ln L_0(\vtheta,\veta) &= -\frac{1}{2}\,[\vy - \mM\cdot\vtheta]\T\cdot\tC\inv\cdot[\vy - \mM\cdot\vtheta] - \frac{1}{2}\ln\det(2\pi\,\tC)
  \quad ,
\end{align}
where the $\vtheta$ dependence is explicit, and the $\veta$ dependence
is implicit through the design matrix $\mM$.
For actual implementation of this log likelihood, we never construct
the exceedingly sparse tensor $\tC$ or $\tC\inv$ explicitly, and we
never call any kind of linear-algebra invert function: The inverse of
$\tC$ is just the $\diag()$ of the inverses.

But we know more than just what's encoded in the basic log-likelihood function $\ln L_0$.
We want to use our knowledge.
One thing we know is that the amplitudes $\sqrt{a_k^2+b_k^2}$
are expected to be higher for the
modes $\nu_k$ closer to the star's characteristic frequecy $\numax$.
One model for this, inspired by the idea that they might be driven
by an effectively Gaussian noise internal to the star, is that the mode
amplitudes $a_k, b_k$ are drawn independently from a Gaussian with zero
mean and variance $H_k$ that depends on mode frequency $\nu_k$ as
\begin{equation}\label{eq:bell}
  H_k = H\,\exp(- \frac{[\nu_k - \numax]^2}{\Gamma^2})
  \quad ,
\end{equation}
where parameters $H, \numax, \Gamma$ set the height, center, and width of
the envelope of mode variances, and the frequencies $\nu_k$ are given
above (\ref{eq:nuk}).
We can pile these into another diagonal variance tensor $\tLambda$---this time
a prior variance not a noise variance
\begin{align}
  \tLambda &\equiv \diag(\begin{bmatrix} \Lambda_0 & H_1 & H_1 & H_2 & H_2 & \hdots & H_K & H_K \end{bmatrix})
  \quad ,
\end{align}
where $\Lambda_0$ is a (small) prior variance for the constant term $a_0$.
Recall that the flux data were normalized to unity and then had unity
(or a filter??) subtracted, to give them mean zero.

Once we have set our prior beliefs about the mode amplitudes using $H,
\numax, \Gamma$, we don't care about the amplitudes' particular values.
Following \citet{gaussianproduct}, we can marginalize out these amplitudes
with a closed-form expression to make the
linear-marginalized log-likelihood function $\ln L_1$:
\begin{align}
  \ln L_1(\veta) &= -\frac{1}{2}\,\vy\T\cdot\tB\inv\cdot\vy - \frac{1}{2}\ln\det(2\pi\,\tB)
  \\
  \tB &= \tC + \mM\cdot\tLambda\cdot\mM\T
  \\
  \veta &\equiv \begin{bmatrix} \Delta\nu & \numax & \nupeak & H & \Gamma & \Lambda_0 & K \end{bmatrix}\T
  \quad ,
\end{align}
where $\ln L_1(\veta)$ depends only on the nonlinear parameters $\veta$ because
the linear parameters $\vtheta$ have been marginalized out,
$\tB$ is a composite variance tensor taking account both the noise
and the prior variance, and the nonlinear parameter list $\veta$ just got
longer.
Once again, implementation of the expression for $\ln L_1$ is not straightforward:
We use the matrix inversion lemma and matrix determinant lemma to simplify each
term to a smaller linear-algebra operation than what's implied by any naive
implementation.

Still in the vector $\veta$ there are too many nonlinear parameters for brute-force
search, so we reduce dimensionality
using known regularities of stars: 
Mode amplitudes are a function of evolutionary
state, where evolutionary state can be represented by the characteristic
frequency $\numax$, or the position of
the star on the color-magnitude diagram or in the space of temperature
and surface gravity.
...
\begin{align}
  H &\leftarrow \mbox{some function of $\numax$ here}
  \\
  \Gamma &\leftarrow \mbox{some function of $\numax$ here}
  \quad ,
\end{align}
... and the log likelihood becomes very insensitive to $K$ once $K$ is large enough;
we set $K=XX$ in all that follows; this is large enough.

... Make $\ln L_2$, marginalized over $\nupeak$ also ...
\begin{align}
  \ln L_2(\numax,\Delta\nu) = \ln \left[\frac{1}{\Delta\nu}\,\int_{\numax-\frac{1}{2}\,\Delta\nu}^{\numax+\frac{1}{2}\,\Delta\nu} \exp\left(\ln L_1(\veta)\right)\,\dd\nupeak\right]
\end{align}
... implementation notes ...

... Comment on the odd-frequency modes? ...

\acknowledgments
It is a pleasure to thank
Andy Casey (Monash),
Stephen Feeney (UCL),
and
Dan Foreman-Mackey (Flatiron)
for valuable work and discussions related to this project.

\software{
\package{Astropy} \citep{astropy, astropy:2018},
\package{IPython} \citep{ipython},
\package{matplotlib} \citep{mpl},
\package{numpy} \citep{numpy}
}

\bibliographystyle{aasjournal}
\bibliography{tess_seismology}

\end{document}


